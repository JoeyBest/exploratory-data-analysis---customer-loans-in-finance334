{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import missingno as msno\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import yaml\n",
    "from scipy import stats\n",
    "from scipy.stats import normaltest\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import inspect\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing the df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def credentials_reader():\n",
    "    with open('credentials.yaml', 'r') as file:\n",
    "        credentials = yaml.safe_load(file)\n",
    "        return credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RDSDatabaseConnector:\n",
    "    def __init__(self, credentials):\n",
    "        self.credentials = credentials\n",
    "        \n",
    "    def initiate_engine(self):\n",
    "\n",
    "        self.DATABASE_TYPE = 'postgresql'\n",
    "        self.DBAPI = 'psycopg2'\n",
    "        self.HOST = 'eda-projects.cq2e8zno855e.eu-west-1.rds.amazonaws.com'\n",
    "        self.USER = 'loansanalyst'\n",
    "        self.PASSWORD = 'EDAloananalyst'\n",
    "        self.DATABASE = 'payments'\n",
    "        self.PORT = 5432\n",
    "\n",
    "        self.engine = create_engine(f\"{self.DATABASE_TYPE}+{self.DBAPI}://{self.USER}:{self.PASSWORD}@{self.HOST}:{self.PORT}/{self.DATABASE}\")\n",
    "        self.engine.execution_options(isolation_level='AUTOCOMMIT').connect()\n",
    "        \n",
    "        inspector = inspect(self.engine)\n",
    "        self.table_inspector = inspector.get_table_names()\n",
    "        print(self.table_inspector)\n",
    "\n",
    "    def database_to_dataframe(self):\n",
    "        ######### maybe add wiith self.engine.connect() as connection: (only if it doesn't work like this)\n",
    "        self.loans = pd.read_sql_table('loan_payments', self.engine)\n",
    "        return self.loans.head(10), self.loans.tail(10)\n",
    "        \n",
    "    # Saves the data to your current pathway as eda.csv\n",
    "    def saves_data_locally(self):\n",
    "        self.loans_df.to_csv('eda.csv', sep=',', index=False, encoding='utf-8')\n",
    "\n",
    "    def load_localdata_to_dataframe(self):\n",
    "        # Defining the columns to read\n",
    "        usecols = [\"id\", \"member_id\",\"loan_amount\", \"funded_amount\", \"funded_amount_inv\", \"term\", \"int_rate\", \"instalment\", \"grade\", \"sub_grade\", \"employment_length\", \"home_ownership\", \"annual_inc\", \"verification_status\", \"issue_date\", \"loan_status\", \"payment_plan\", \"purpose\", \"dti\", \"delinq_2yrs\", \"earliest_credit_line\", \"inq_last_6mths\", \"mths_since_last_record\", \"open_accounts\", \"total_accounts\", \"out_prncp\", \"out_prncp_inv\", \"total_payment\", \"total_rec_int\", \"total_rec_late_fee\", \"recoveries\", \"collection_recovery_fee\", \"last_payment_date\", \"last_payment_amount\", \"next_payment_date\", \"last_credit_pull_date\", \"collections_12_mths_ex_med\", \"mths_since_last_major_derog\", \"policy_code\", \"application_type\"]\n",
    "    # Read data with subset of columns\n",
    "        loan_data_df = pd.read_csv(\"/Users/joeybest/Ai Core/EDA/exploratory-data-analysis---customer-loans-in-finance334/eda.csv\", index_col=\"id\", usecols=usecols)\n",
    "        return loan_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads credentials from yaml file\n",
    "credentials = credentials_reader()\n",
    "\n",
    "RDSDatabaseConnector(credentials)\n",
    "# next creates instance of the rds connector\n",
    "loan_data = RDSDatabaseConnector(credentials)\n",
    "loan_data.initiate_engine()\n",
    "loan_data.database_to_dataframe()\n",
    "\n",
    "extracted_data_frame = loan_data.database_to_dataframe()\n",
    "# print(extracted_data_frame)\n",
    "\n",
    "# was after extracted dataframe variable but it seems to work without it:\n",
    "# pd.DataFrame\n",
    "\n",
    "loan_data.save_to_csv(\"eda.csv\")\n",
    "# saves CSV file\n",
    "\n",
    "table_of_loans = loan_data.load_localdata_to_dataframe()\n",
    "print(table_of_loans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_of_loans = loan_data.load_localdata_to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(table_of_loans.head(10))\n",
    "print(table_of_loans.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(table_of_loans.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(table_of_loans.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"std loan_amount = {table_of_loans['loan_amount'].std()}, mean loan_amount = {table_of_loans['loan_amount'].mean()}\")\n",
    "print(f\"Max loan: {table_of_loans['loan_amount'].max()}, Min loan:{table_of_loans['loan_amount'].min()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFrameInfo:\n",
    "\n",
    "    def __init__(self, df_info):\n",
    "        self.df_info = df_info\n",
    "\n",
    "    def check_column_datatypes(self): \n",
    "        return self.df_info.dtypes\n",
    "\n",
    "    def extract_statistical_values(self, columns):\n",
    "        for col in columns:\n",
    "            print(f'Statistics for Col: {col}')\n",
    "            return self.df_info[col].describe()\n",
    "\n",
    "    def count_unique_categories(self, columns):\n",
    "        self.columns = self.df_info.select_dtypes(include=['category']).columns\n",
    "        df_unique = self.df_info[columns[:]]\n",
    "        return df_unique.nunique()\n",
    "        # orrrrrrrr\n",
    "    #def count_unique_categories(self, df_info: pd.DataFrame, column_name: str):\n",
    "     #   return len(df_info[column_name].unique())\n",
    "\n",
    "    def shape_of_dataframe(self):\n",
    "        print(f'Shape of DataFrame: [{self.df_info.shape[0]} rows x {self.df_info.shape[1]} columns]\\n')\n",
    "\n",
    "    def num_of_nulls(self):\n",
    "        cols = self.df_info.columns\n",
    "        # Define an empty list of null column rows\n",
    "        null_coulmn_rows = []\n",
    "        # Populate list of null column rows\n",
    "        for col in cols:\n",
    "            # if self.df[col].isnull().sum() > 0:\n",
    "            null_coulmn_rows.append([col, self.df_info[col].count(), 100*(self.df_info[col].isnull().sum()/len(self.df_info))])\n",
    "        \n",
    "        # Convert the list into dataframe rows\n",
    "        data = pd.DataFrame(null_coulmn_rows)\n",
    "        # Add columns headers\n",
    "        data.columns = ['column', 'count', '% null count']  \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_df = DataFrameInfo(table_of_loans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_df.check_column_datatypes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The no. of Unique Items in {df_df.count_unique_categories(['grade'])}\")\n",
    "print('')\n",
    "print(df_df.extract_statistical_values(['loan_amount']))\n",
    "print(' ')\n",
    "print(df_df.shape_of_dataframe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_df.num_of_nulls())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforming pt 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransform:\n",
    "\n",
    "    def __init__(self, df_info):\n",
    "        self.df_info = df_info\n",
    "\n",
    "    def to_interger(self, column):\n",
    "        for col in column:\n",
    "            self.df_info[col] = self.df_info[col].fillna(0).astype('int32')\n",
    "            # converts to int and fills NULL with 0\n",
    "\n",
    "    def to_boolean(self, column_name):\n",
    "        mask = {'n': False, 'y': True}\n",
    "        self.df_info[column_name].map(mask)\n",
    "        self.df_info[column_name] = self.df_info[column_name].astype('bool')\n",
    "        print(self.df_info[column_name].unique())\n",
    "\n",
    "    def to_object(self, column):\n",
    "        for col in column:\n",
    "            self.df_info[col] = self.df_info[col].astype(object)\n",
    "    \n",
    "    def to_float(self, column):\n",
    "        for col in column:\n",
    "            self.df_info[col] = self.df_info[col].astype('float64')\n",
    "\n",
    "    def to_rounded_float(self, column, decimal_places):\n",
    "        self.df_info[column] = self.df_info[column].apply(lambda x: round(x, decimal_places))\n",
    "\n",
    "    def to_category(self, column):\n",
    "        for col in column:\n",
    "            self.df_info[col] = self.df_info[col].astype('category')\n",
    "\n",
    "    def to_numerical_column(self, column):\n",
    "        for col in column:\n",
    "            pd.to_numeric(self.df_info[col])\n",
    "\n",
    "    def extract_integer_from_string(self, column):\n",
    "        for col in column:\n",
    "            self.df_info[col] = self.df_info[col].str.extract('(\\d+)')\n",
    "\n",
    "    def strings_to_dates(self, column):\n",
    "        for col in column:\n",
    "            self.df_info[col] = pd.to_datetime(self.df_info[col], errors='coerce', format=\"%b-%Y\")#.dt.to_period('M') - method converts the datetime to a period (M) which is a date that contains only the month and year since this is the resolution of the data provided.\n",
    "\n",
    "    def replace_string_text(self, column_name, original_string: str, new_string: str):\n",
    "        self.df_info[column_name].replace(original_string, new_string)\n",
    "\n",
    "    def rename(self, column_name, new_column_name):\n",
    "        self.df_info.rename(columns={col_name: new_col_name})\n",
    "\n",
    "    def drop_column(self, column):\n",
    "        for col in column:\n",
    "            self.df_info.drop(col, axis=1, inplace=True)\n",
    "\n",
    "    def remove_null_rows(self, column_name):\n",
    "        '''\n",
    "        This method is used to remove rows within the dataframe where data points from a specified column are null.\n",
    "        '''\n",
    "        self.df_info.dropna(subset=column_name, inplace=True)\n",
    "\n",
    "    def save_transformed_data(self, filename='full_loan_data.csv'):\n",
    "        self.df_info.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ ==  \"__main__\":\n",
    "    table_of_loans = pd.read_csv('eda.csv')\n",
    "    Transform = DataTransform(table_of_loans)\n",
    "\n",
    "    Transform.to_boolean('payment_plan')\n",
    "    # transforms n to false and y to true in the payment plan column\n",
    "\n",
    "    to_object_columns = ['id', 'member_id', 'policy_code']\n",
    "    Transform.to_object(to_object_columns)\n",
    "    \n",
    "    convert_categories = ['grade', 'sub_grade', 'home_ownership', 'verification_status', 'loan_status', 'purpose', 'employment_length']\n",
    "    Transform.to_category(convert_categories)\n",
    "    # transforms column values to catagories\n",
    "\n",
    "    string_month_and_year = ['last_credit_pull_date', 'next_payment_date', 'last_payment_date', 'earliest_credit_line', 'issue_date']\n",
    "    Transform.strings_to_dates(string_month_and_year)\n",
    "    # transforms strings to a datetime format MonthYear\n",
    "\n",
    "    string_numbers = ['term']\n",
    "    Transform.extract_integer_from_string(string_numbers)\n",
    "\n",
    "    numericals = ['term', 'mths_since_last_record', 'mths_since_last_major_derog', 'mths_since_last_delinq', 'mths_since_last_record']\n",
    "    Transform.to_numerical_column(numericals)\n",
    "\n",
    "    drop_cols = ['funded_amount', 'application_type', 'policy_code', 'out_prncp_inv', 'total_payment_inv']\n",
    "    Transform.drop_column(drop_cols)\n",
    "    # funded_amount is missing some data but contains the same data as funded_amount_inv, so we can drop it\n",
    "    # out_prncp_inv contains the same data as out_prncp, so we can drop it\n",
    "    # total_payment_inv contains the same data as total_payment, so we can drop it\n",
    "    # application_type and policy_code are the same for everyone and doesn't provide us with much info\n",
    "    \n",
    "    int_numbers = ['loan_amount', 'funded_amount_inv', 'annual_inc', 'term', 'open_accounts', 'total_accounts', 'collections_12_mths_ex_med', 'mths_since_last_delinq', 'mths_since_last_major_derog']\n",
    "    Transform.to_interger(int_numbers)\n",
    "    \n",
    "    Transform.to_rounded_float('collection_recovery_fee', 2)\n",
    "\n",
    "    # saves a new CSV of the df called 'full_loan_data.csv'\n",
    "    Transform.save_transformed_data('full_loan_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforming pt 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFrameTransform:\n",
    "\n",
    "    def __init__(self, df_transform):\n",
    "        self.df_transform = df_transform\n",
    "\n",
    "    def num_of_nulls(self):\n",
    "        cols = self.df_transform.columns\n",
    "        # Define an empty list of null column rows\n",
    "        null_coulmn_rows = []\n",
    "        # Populate list of null column rows\n",
    "        for col in cols:\n",
    "            # if self.df[col].isnull().sum() > 0:\n",
    "            null_coulmn_rows.append([col, self.df_transform[col].count(), 100*(self.df_transform[col].isnull().sum()/len(self.df_transform))])\n",
    "        \n",
    "        # Convert the list into dataframe rows\n",
    "        data = pd.DataFrame(null_coulmn_rows)\n",
    "        # Add columns headers\n",
    "        data.columns = ['column', 'count', '% null count']  \n",
    "        return data\n",
    "\n",
    "    def drop_column(self, column):\n",
    "        '''\n",
    "        This method removes the listed columns from the dataframe.\n",
    "        '''\n",
    "        for col in column:\n",
    "            self.df_transform.drop(col, axis=1, inplace=True)\n",
    "\n",
    "    def drop_null_rows(self, column):\n",
    "        self.df_transform.dropna(subset=column, inplace=True)\n",
    "    \n",
    "    def impute_zeros(self, column):\n",
    "        for col in column:\n",
    "            self.df_transform[col] = self.df_transform[col].fillna(0)\n",
    "\n",
    "    def impute_median(self, column):\n",
    "        for col in column:\n",
    "            self.df_transform[col] = self.df_transform[col].fillna(self.df_transform[col].median())\n",
    "\n",
    "    def impute_mean(self, column):\n",
    "        for col in column:\n",
    "            self.df_transform[col] = self.df_transform[col].fillna(self.df_transform[col].mean())\n",
    "\n",
    "    def log_transform(self, column):\n",
    "        for col in column:\n",
    "            log_sample = self.df_transform[col] = self.df_transform[col].map(lambda i: np.log(i) if i > 0 else 0)\n",
    "            t=sns.histplot(log_sample,label=\"Skewness: %.2f\"%(log_sample.skew()) )\n",
    "            t.legend()\n",
    "    \n",
    "    def box_cox_transform(self, column):\n",
    "        for col in column:\n",
    "            boxcox_column = self.df_transform[col] + 0.01\n",
    "            a, b = stats.boxcox(boxcox_column)\n",
    "            self.df_transform[col] = a \n",
    "            t=sns.histplot(boxcox_column,label=\"Skewness: %.2f\"%(boxcox_column.skew()) )\n",
    "            t.legend()\n",
    "\n",
    "    def remove_outliers_iqr(self, data, threshold=1.5):\n",
    "        q1 = np.percentile(data, 25)\n",
    "        q3 = np.percentile(data, 75)\n",
    "        iqr = q3 - q1\n",
    "        lower_bound = q1 - threshold * iqr\n",
    "        upper_bound = q3 + threshold * iqr\n",
    "        filtered_data = data[(data >= lower_bound) & (data <= upper_bound)]\n",
    "        return filtered_data\n",
    "\n",
    "    def remove_outliers_iqr_dataframe(self, column, threshold=1.5):\n",
    "        filtered_dataframe = self.df_transform\n",
    "        for col in column:\n",
    "            filtered_dataframe[col] = self.remove_outliers_iqr(filtered_dataframe[col], threshold)\n",
    "        return filtered_dataframe\n",
    "        \n",
    "    def save_transformed_data(self, filename='transformed_loan_data.csv'):\n",
    "        self.df_transform.to_csv(filename, index=False)\n",
    "\n",
    "    def save_untransformed_data(self, filename='untransformed_loan_data.csv'):\n",
    "        '''\n",
    "        This method is used to save the current dataframe as a new CSV file called 'transformed_loan_data.csv'\n",
    "        '''\n",
    "        self.df_transform.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plotter:\n",
    "\n",
    "    def __init__(self, df_transform):\n",
    "        self.df_transform = df_transform\n",
    "\n",
    "    def agostino_k2_test(self, col):\n",
    "        stat, p = normaltest(self.df_transform[col], nan_policy='omit')\n",
    "        print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
    "\n",
    "    def histogram(self, col):\n",
    "        self.df_transform[col].hist(bins=40)\n",
    "\n",
    "    def density_plot(self, col):\n",
    "        sns.histplot(data=self.df_transform[col], kde=True)\n",
    "        sns.despine()\n",
    "\n",
    "    def boxplot(self, col):\n",
    "        fig = px.box(self.df_transform[col],width=600, height=500)\n",
    "        fig.show()\n",
    "\n",
    "    def bar(self, data):\n",
    "        df = pd.DataFrame(data, index=[0])\n",
    "        sns.barplot(data=df)\n",
    "\n",
    "    def scatter(self, col):\n",
    "        sns.scatterplot(self.df_transform[col])\n",
    "\n",
    "    def pie(self, col):\n",
    "        data = self.df_transform[col].value_counts()\n",
    "\n",
    "        fig = px.pie(values=data.values, names=data.index, title= 'Pie Chart of {self.df_transform[col]}', width=600)\n",
    "        fig.show()\n",
    "\n",
    "    def qq_plot(self, col):\n",
    "        self.df_transform.sort_values(by=col, ascending=True)\n",
    "        qq_plot = qqplot(self.df_transform[col], scale=1, line='q')\n",
    "        plt.show()\n",
    "\n",
    "    def show_missing_nulls(self):\n",
    "        msno.matrix(self.df_transform)\n",
    "        plt.show()\n",
    "\n",
    "    def heatmap(self, col):\n",
    "        corr = self.df_transform[col].corr()\n",
    "        mask = np.zeros_like(corr)\n",
    "        mask[np.triu_indices_from(mask)] = True\n",
    "        plt.figure(figsize=(10, 8))\n",
    "\n",
    "        cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "        sns.heatmap(corr, mask=mask, square=True, linewidths=5,\n",
    "                    annot=True, cmap=cmap)\n",
    "\n",
    "        # Show the plot\n",
    "        plt.show()\n",
    "\n",
    "    def multi_plot(self, col):\n",
    "        print(f'The median of {[col]} is {self.df_transform[col].median()}')\n",
    "        print(f'The mean of {[col]} is {self.df_transform[col].mean()}')\n",
    "        print(f\"Skew of {[col]} column is {self.df_transform[col].skew()}\")\n",
    "        stat, p = normaltest(self.df_transform[col], nan_policy='omit')\n",
    "        print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
    "        self.df_transform[col].hist(bins=40)\n",
    "        sns.histplot(data=self.df_transform[col], kde=True)\n",
    "        sns.despine()\n",
    "        self.df_transform.sort_values(by=col, ascending=True)\n",
    "        qq_plot = qqplot(self.df_transform[col], scale=1, line='q', fit=True)\n",
    "        plt.show()\n",
    "\n",
    "    def find_skew(self, col):\n",
    "        if self.df_transform[col].skew() >= 0.86:\n",
    "            print('Skewed!')\n",
    "            print(f' {self.df_transform[col].skew()} is over 0.85')\n",
    "        else:\n",
    "            print(f' {self.df_transform[col].skew()} is under 0.85')\n",
    "\n",
    "    def multi_hist_plot(self, num_cols):\n",
    "        sns.set(font_scale=0.7)\n",
    "        f = pd.melt(self.df_transform, value_vars=num_cols)\n",
    "        g = sns.FacetGrid(f, col=\"variable\", col_wrap=4,\n",
    "                          sharex=False, sharey=False)\n",
    "        g = g.map(sns.histplot, \"value\", kde=True)\n",
    "        plt.show()\n",
    "\n",
    "    def multi_qq_plot(self, cols):\n",
    "        remainder = 1 if len(cols) % 4 != 0 else 0\n",
    "        rows = int(len(cols) / 4 + remainder)\n",
    "\n",
    "        fig, axes = plt.subplots(\n",
    "            ncols=4, nrows=rows, sharex=False, figsize=(12, 6))\n",
    "        for col, ax in zip(cols, np.ravel(axes)):\n",
    "            sm.qqplot(self.df_transform[col], line='s', ax=ax, fit=True)\n",
    "            ax.set_title(f'{col} QQ Plot')\n",
    "        plt.tight_layout()\n",
    "\n",
    "    def show_outliers(self):\n",
    "        #select only the numeric columns in the DataFrame\n",
    "        df = self.df_transform.select_dtypes(include=['float64'])\n",
    "        plt.figure(figsize=(18,14))\n",
    "\n",
    "        for i in list(enumerate(df.columns)):\n",
    "            fig_cols = 4\n",
    "            fig_rows = int(len(df.columns)/fig_cols) + 1\n",
    "            plt.subplot(fig_rows, fig_cols, i[0]+1)\n",
    "            sns.boxplot(data=df[i[1]]) \n",
    "\n",
    "        # Show the plot\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def show_outliers_after_removal(self, dataframe, columns):\n",
    "        plt.figure(figsize=(18, 14))\n",
    "\n",
    "        for i, col in enumerate(columns):\n",
    "            fig_cols = 4\n",
    "            fig_rows = len(columns) // fig_cols + 1\n",
    "            plt.subplot(fig_rows, fig_cols, i + 1)\n",
    "            sns.boxplot(data=dataframe[col])\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_of_loans = pd.read_csv('full_loan_data.csv')\n",
    "df_cols = DataFrameTransform(table_of_loans)\n",
    "plot = Plotter(table_of_loans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_cols.num_of_nulls())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cols.impute_zeros(['employment_length'])\n",
    "# missing values can be assumed to be unemployment\n",
    "df_cols.impute_median(['int_rate'])\n",
    "# Missing values imputed to median values \n",
    "df_cols.drop_null_rows(['last_payment_date', 'last_credit_pull_date'])\n",
    "# Data missing too small to impact much, so remove rows\n",
    "df_cols.drop_column(['mths_since_last_delinq', 'next_payment_date', 'mths_since_last_record', 'mths_since_last_major_derog'])\n",
    "# Data missing from 57%-88%, so drop column\n",
    "# df_cols.drop_column(['funded_amount', 'application_type', 'policy_code', 'out_prncp_inv', 'total_payment_inv'])\n",
    "# columns that shouldve been dropped in DataTransformation stage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cols.save_untransformed_data('untransformed_loan_data.csv')\n",
    "# to save a version of untransformed data for use in Milestone 4 later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing int_rate\n",
    "int_rate_data = table_of_loans['int_rate']\n",
    "\n",
    "# D’Agostino’s K^2 Test\n",
    "stat, p = normaltest(int_rate_data, nan_policy='omit')\n",
    "print('Statistics=%.3f, p=%.3f' % (stat, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.show_missing_nulls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualising the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.multi_plot('total_payment')\n",
    "# positive skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.multi_plot('funded_amount_inv')\n",
    "# normal distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.multi_plot('instalment')\n",
    "# skewed but not badly, so could be normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.multi_plot('loan_amount')\n",
    "# positive skew ?\n",
    "# normal distribution?, slight positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.multi_plot('int_rate')\n",
    "# distribution is normal and  mean and median are prettyy similar, so we'll impute using the median\n",
    "# normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change around columns as you please"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skew data\n",
    "# normal skew?\n",
    "# Skew of int_rate column is 0.45661978102982004/// logt= -0.38/// boxcox = 0.46\n",
    "# Skew of total_accounts column is 0.7790400286166349/// logt= -0.68/// boxcox = 0.78\n",
    "# Skew of loan_amount column is 0.8049220181025606/// logt= -0.68/// boxcox = 0.80\n",
    "# Skew of funded_amount_inv column is 0.8133743608921553/// logt= -3.33/// boxcox = 0.81\n",
    "# Skew of dti column is 0.18910057534680505/// logt= -1.98/// boxcox = 0.19\n",
    "\n",
    "# positive skew:\n",
    "# Skew of instalment column is 0.9965721690437305/// logt= -0.67/// # ? boxcox = 1\n",
    "# Skew of open_accounts column is 1.0591835802494733/// logt= -0.47/// # boxcox = 1.06\n",
    "# Skew of total_rec_prncp column is 1.2626785390807123/// logt= -0.92/// # boxcox = 1.26\n",
    "# Skew of total_payment column is 1.2698752968451772/// logt= -0.75/// # boxcox = 1.27\n",
    "# Skew of total_rec_int column is 2.2045848938701638/// logt= -0.56/// # boxcox = 2.20\n",
    "# Skew of out_prncp column is 2.35405079479137/// logt=0.57/// # boxcox = 2.35\n",
    "# Skew of last_payment_amount column is 2.4972203995928135/// logt=0.13/// # boxcox = 2.50\n",
    "# Skew of inq_last_6mths column is 3.253522804381087/// logt=1.97///# boxcox = 3.25\n",
    "# Skew of delinq_2yrs column is 5.376384516017939 /// # logt=5.42/// boxcox = 5.38\n",
    "# Skew of annual_inc column is 8.717499091755876/// logt= 0.14/// # boxcox = 8.72\n",
    "# Skew of total_rec_late_fee column is 13.174553220535644/// logt=5.51///# boxcox = 13.17\n",
    "# Skew of recoveries column is 14.389298686317593/// logt=3.82/// # boxcox = 14.39\n",
    "# Skew of collections_12_mths_ex_med column is 20.307695011511576/// # logt=65.06/// boxcox = 20.31\n",
    "# Skew of collection_recovery_fee column is 27.807756579876514/// logt=5.41/// # boxcox = 27.81"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logt_cols = ['annual_inc', 'total_accounts', 'open_accounts', 'last_payment_amount']\n",
    "df_cols.log_transform(logt_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxcox_cols = ['loan_amount', 'instalment', 'int_rate', 'dti', 'funded_amount_inv', 'total_payment']\n",
    "df_cols.box_cox_transform(boxcox_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code below saves file, but its already saved\n",
    "df_cols.save_transformed_data('transformed_loan_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlier Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_loans = pd.read_csv('transformed_loan_data.csv')\n",
    "df_cols = DataFrameTransform(transformed_loans)\n",
    "plot = Plotter(transformed_loans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = ['loan_amount','funded_amount_inv', 'int_rate', 'instalment', 'dti', 'annual_inc', 'total_payment', 'total_accounts', 'open_accounts', 'last_payment_amount']\n",
    "plot.multi_hist_plot(numerical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.multi_qq_plot(numerical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot of \n",
    "plot.show_outliers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.boxplot(numerical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df_cols.remove_outliers_iqr_dataframe(column= numerical_cols, threshold=1.5)\n",
    "plot.show_outliers_after_removal(dataframe=filtered_df, columns=numerical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_without_outliers = DataFrameTransform(filtered_df)\n",
    "plot_without_outliers = Plotter(filtered_df)\n",
    "Transform = DataTransform(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_without_outliers.boxplot(numerical_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_without_outliers.multi_hist_plot(numerical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_without_outliers.show_missing_nulls()\n",
    "# removing the outliers has left NAN values, so we will either transform or remove the rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_without_outliers.num_of_nulls()\n",
    "# values are all very low, impute with the mean!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outliers: If a column has noticeable outliers, the median is more robust and less influenced by extreme values.\n",
    "to_be_median_imputed = ['loan_amount', 'funded_amount_inv',  'int_rate', 'instalment', 'annual_inc', 'open_accounts', 'total_accounts', 'total_payment', 'last_payment_amount']\n",
    "df_without_outliers.impute_median(to_be_median_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_without_outliers.num_of_nulls()\n",
    "plot_without_outliers.show_missing_nulls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_without_outliers.multi_qq_plot(numerical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_without_outliers.heatmap(numerical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Threshold for highly correlated columns is 0.85\n",
    "\n",
    "# Multi-linearity between 'loan_amount', 'instalment' & 'funded_amount_inv'\n",
    "\n",
    "# funded_amount_inv and loan_amount corrolation = 0.96\n",
    "# instalmannt and loan_amount corrolation = 0.96\n",
    "# instalmannt and funded_amount_inv corrolation = 0.93\n",
    "    # desptie columns being past the threshold, they're all important for the analysis stage, so we wont be dropping any of them!\n",
    "\n",
    "# total_payment is highly corrolated with loan_amount, funded_amount_inv and instalment too, but only at 0.81, 0.78 and 0.81 respectively\n",
    "# therefore not passed the 0.85 threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cols.save_transformed_data('filtered_loan_data.csv')\n",
    "# saves data without any outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current state of the Loans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = pd.read_csv('untransformed_loan_data.csv')\n",
    "# untransformed data, specifically for milestone 4\n",
    "df_without_outliers = DataFrameTransform(filtered_df)\n",
    "plot_without_outliers = Plotter(filtered_df)\n",
    "Transform = DataTransform(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df['term'] = filtered_df['term'].replace(0, 36)\n",
    "# replaces 0's in term with median value(36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loan recovered (total_payment) againts invstor funding (funded_amount_inv) & total amount funded (loan_amount)\n",
    "total_amount_funded = filtered_df['loan_amount'].sum()\n",
    "print(f' Total loan amount funded is {total_amount_funded}')\n",
    "invstor_funding = filtered_df['funded_amount_inv'].sum()\n",
    "print(f' Total amount invested is {invstor_funding}')\n",
    "total_payment_sum = filtered_df['total_payment'].sum()\n",
    "print(f' Total payment recovered is {total_payment_sum}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#together and rounded\n",
    "totals = round(filtered_df[['loan_amount', 'funded_amount_inv', 'total_payment']].sum(), 0)\n",
    "totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % of loans recovered\n",
    "print(f'Percentage of the loans recovered against the investor_funding is {(total_payment_sum/invstor_funding)*100}')\n",
    "print(f'Percentage of the loans recovered against the total_amount_funded is {(total_payment_sum/total_amount_funded)*100}')\n",
    "\n",
    "per_of__inv_loan_recovered = round((total_payment_sum/invstor_funding)*100, 2)\n",
    "per_of__total_loan_recovered = round((total_payment_sum/total_amount_funded)*100, 2)\n",
    "print(f' {per_of__inv_loan_recovered}%')\n",
    "print(f' {per_of__total_loan_recovered}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "'funded_amount_inv': (filtered_df['total_payment'].sum()/invstor_funding)*100,\n",
    "'loan_amount': (filtered_df['total_payment'].sum()/total_amount_funded)*100\n",
    "}\n",
    "\n",
    "plot_without_outliers.bar(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate percentage of investor funding recovered\n",
    "pct_invetor_rec = round(100 * totals.total_payment/totals.funded_amount_inv, 2)\n",
    "# Calculate percentage of funded amount recovered\n",
    "pct_total_rec = round(100 * totals.total_payment/totals.loan_amount, 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Funding': ['Investor', 'Total'],\n",
    "            'Percent': [pct_invetor_rec, pct_total_rec]}\n",
    "\n",
    "# Create a Pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "display(df)\n",
    "\n",
    "df.plot(x=\"Funding\", y=\"Percent\", kind=\"bar\", ylabel=\"% Recovered\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(y=\"Percent\", kind=\"pie\", ylabel=\"% Recovered\", labels=df['Funding'], startangle=90, xlabel='Funding')\n",
    "# Add a title and xlabel\n",
    "plt.title(\"Recovery Percentage by Funding Source\")\n",
    "plt.legend(title=\"Funding Source\")\n",
    "# Display the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_amount = round(total_amount_funded - total_payment_sum, 2)\n",
    "remaining_amount\n",
    "# whats left to be paid overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a DataFrame with the recovered and remaining amounts\n",
    "data = {'Amount': [total_payment_sum, remaining_amount]}\n",
    "df = pd.DataFrame(data, index=['Recovered', 'Remaining'])\n",
    "\n",
    "# Plotting the pie chart\n",
    "df.plot(y='Amount', kind='pie', autopct='%1.1f%%', startangle=90)\n",
    "# Adding a title and legend title\n",
    "plt.title(\"Percentage of Amount Recovered\")\n",
    "plt.legend(title=\"Key\")\n",
    "# Displaying the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum of the total_payment and the instalment and multiply this by 6. \n",
    "# Then divide this by the sum of the funded amount inv. \n",
    "# times By 100 to get this as a percentage\n",
    "\n",
    "six_month_projection = round((filtered_df[\"total_payment\"].sum()+(filtered_df[\"instalment\"].sum()*6))/(filtered_df[\"funded_amount_inv\"].sum())*100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Funding': ['Total'],\n",
    "        'Percent': [six_month_projection]}\n",
    "\n",
    "# Create a Pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "display(df)\n",
    "\n",
    "df.plot(x=\"Funding\", y=\"Percent\", kind=\"bar\", ylabel=\"% Recovered\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame to include only charged off loans\n",
    "charged_off_loans = filtered_df[filtered_df['loan_status'] == 'Charged Off']\n",
    "\n",
    "# Calculate the percentage of charged off loans\n",
    "charged_off_percentage = (charged_off_loans.shape[0] / filtered_df.shape[0]) * 100\n",
    "\n",
    "# Calculate the total amount paid towards charged off loans\n",
    "total_payment_charged_off = charged_off_loans['total_payment'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Percentage of charged off loans: {charged_off_percentage:.2f}%\")\n",
    "print(f\"Total amount paid towards charged off loans: {total_payment_charged_off:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "charged_off_loans_loan_amount_sum = round(charged_off_loans['loan_amount'].sum(),2)\n",
    "print(f' Total amount to be paid: {charged_off_loans_loan_amount_sum}')\n",
    "\n",
    "charged_off_loans_total_payment_sum = round(charged_off_loans['total_payment'].sum(),2)\n",
    "print(f' Total that has been paid: {charged_off_loans_total_payment_sum}')\n",
    "\n",
    "charged_off_loans_loss = round(charged_off_loans_loan_amount_sum - charged_off_loans_total_payment_sum, 2)\n",
    "print(f' Total that has been lost: {charged_off_loans_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Paid': [charged_off_loans_total_payment_sum],\n",
    "        'Loss': [charged_off_loans_loss]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a pie chart\n",
    "labels = ['Paid', 'Loss']\n",
    "sizes = [charged_off_loans_total_payment_sum, charged_off_loans_loss]\n",
    "colors = ['#1f77b4', '#ff7f0e']\n",
    "plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%')\n",
    "# Setting the title of the pie chart\n",
    "plt.title('Paid vs. Loss')\n",
    "# Displaying the pie chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a pie chart\n",
    "labels = ['Paid', 'Loss']\n",
    "sizes = [charged_off_loans_total_payment_sum, charged_off_loans_loss]\n",
    "colors = ['#1f77b4', '#ff7f0e']\n",
    "plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "# Setting the title of the pie chart\n",
    "plt.title('Paid vs. Loss')\n",
    "# Adding a legend\n",
    "plt.legend()\n",
    "# Displaying the pie chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df['num_of_payments_made'] = filtered_df['total_payment'] / filtered_df['instalment']\n",
    "filtered_df['months_left_to_pay'] = filtered_df['term'] - filtered_df['num_of_payments_made']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering the DataFrame to include only late loans\n",
    "risk_loans = filtered_df[(filtered_df['loan_status'] == 'Late (31-120 days)') | (filtered_df['loan_status'] == 'Late (16-30 days)')]\n",
    "# Calculating the percentage of risk loans\n",
    "risk_percentage = round((risk_loans.shape[0] / filtered_df.shape[0]) * 100, 2)\n",
    "# Calculating the total amount paid towards charged off loans\n",
    "total_customers_in_risk_bracket = round(risk_loans.shape[0],0)\n",
    "print(f\"Percentage of risk loans: {risk_percentage}%\")\n",
    "print(f\"Total number of customers in risk bracket: {total_customers_in_risk_bracket}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_loans_loan_amount_sum = round(risk_loans['loan_amount'].sum(),2)\n",
    "# print(f' Total amount to be paid: {risk_loans_loan_amount_sum}')\n",
    "risk_loans_total_payment_sum = round(risk_loans['total_payment'].sum(),2)\n",
    "# print(f' Total that has been paid: {risk_loans_total_payment_sum}')\n",
    "risk_loans_loss = round(risk_loans_loan_amount_sum - risk_loans_total_payment_sum, 2)\n",
    "print(f'Total still to be paid (exclusive of int_rate): {risk_loans_loss}')\n",
    "# print(f'Percentage: {round((risk_loans_loss/risk_loans_loan_amount_sum)*100,2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_amount_left_to_pay_ = risk_loans['months_left_to_pay'] * risk_loans['instalment']\n",
    "projected_loss = round(risk_amount_left_to_pay_.sum(), 2)\n",
    "print(f\"Projected Loss if Switched to Charged Off: ${projected_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_charged_off_rev_pct = risk_percentage + charged_off_percentage \n",
    "print(f'Percentage of late and charged off revenue: {risk_charged_off_rev_pct:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare grade, purpose and home_ownership with \n",
    "# customers who have already stopped paying \n",
    "# and \n",
    "# customers who are currently behind on payments.\n",
    "\n",
    "# im also going to investigate employment_length, sub_grade, annual_inc and open_accounts\n",
    "\n",
    "# Does the grade of the loan have effect on customers not paying?\n",
    "# Is the purpose for the loan likely to have an effect?\n",
    "# Does the home_ownership value contribute to the likelihood a customer won't pay?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paid_loans = filtered_df[(filtered_df['loan_status'] == 'Fully Paid')]\n",
    "on_time_loans = filtered_df[(filtered_df['loan_status'] == 'Current')]\n",
    "grace_period_loans = filtered_df[(filtered_df['loan_status'] == 'In Grace Period')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Paid Grades': paid_loans['grade'].value_counts(),\n",
    "        # 'On Time Grades': on_time_loans['grade'].value_counts(),\n",
    "        # 'Grace Period Grades': grace_period_loans['grade'].value_counts(),\n",
    "        'Risk Loan Grades': risk_loans['grade'].value_counts(),\n",
    "        'Charged Off Grades': charged_off_loans['grade'].value_counts()\n",
    "        }\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    print(f\"Paid Loans {index} Grade: {row['Paid Grades']}\")\n",
    "    # print(f\"On time {index} Grades: {row['On Time Grades']}\")\n",
    "    # print(f\"Grace Period {index} Grades: {row['Grace Period Grades']}\")\n",
    "    print(f\"Risk Loan {index} Grades: {row['Risk Loan Grades']}\")\n",
    "    print(f\"Charged Off Loan {index} Grades: {row['Charged Off Grades']}\") \n",
    "\n",
    "df.plot(kind='bar', stacked=True)\n",
    "\n",
    "plt.title('Counts of Grades')\n",
    "plt.xlabel('Grade')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "# Displaying the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Paid sub grade': paid_loans['sub_grade'].value_counts(),\n",
    "        # 'On Time sub grade': on_time_loans['sub_grade'].value_counts(),\n",
    "        # 'Grace Period sub grade': grace_period_loans['sub_grade'].value_counts(),\n",
    "        'Risk Loan sub grade': risk_loans['sub_grade'].value_counts(),\n",
    "        'Charged Off sub grade': charged_off_loans['sub_grade'].value_counts()\n",
    "        }\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    print(f\"Paid Loans sub grade {index}: {row['Paid sub grade']}\")\n",
    "    # print(f\"On time sub grade {index}: {row['On Time sub grade']}\")\n",
    "    # print(f\"Grace Period sub grade {index}: {row['Grace Period sub grade']}\")\n",
    "    print(f\"Risk Loan sub grade {index}: {row['Risk Loan sub grade']}\")\n",
    "    print(f\"Charged Off Loan sub grade {index}: {row['Charged Off sub grade']}\")\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df.plot(kind='bar', stacked=True)\n",
    "\n",
    "plt.title('Count of Sub Grade')\n",
    "plt.xlabel('Sub Grades')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next plot\n",
    "data = {'Paid purpose': paid_loans['purpose'].value_counts(),\n",
    "        # 'On Time purpose': on_time_loans['purpose'].value_counts(),\n",
    "        # 'Grace Period purpose': grace_period_loans['purpose'].value_counts(),\n",
    "        'Risk Loan purpose': risk_loans['purpose'].value_counts(),\n",
    "        'Charged Off purpose': charged_off_loans['purpose'].value_counts()\n",
    "        }\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    print(f\"Paid Loans purpose: {index} {row['Paid purpose']}\")\n",
    "    # print(f\"On time purpose: {index} {row['On Time purpose']}\")\n",
    "    # print(f\"Grace Period purpose: {index} {row['Grace Period purpose']}\")\n",
    "    print(f\"Risk Loan purpose: {index} {row['Risk Loan purpose']}\")\n",
    "    print(f\"Charged Off Loan purpose: {index} {row['Charged Off purpose']}\")\n",
    "\n",
    "df.plot(kind='bar', stacked=True)\n",
    "\n",
    "plt.title('Counts of Purposes')\n",
    "plt.xlabel('Loan Purpose')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next plot\n",
    "data = {'Paid home ownership': paid_loans['home_ownership'].value_counts(),\n",
    "    # 'On Time home ownership': on_time_loans['home_ownership'].value_counts(),\n",
    "    # 'Grace Period home ownership': grace_period_loans['home_ownership'].value_counts(),\n",
    "    'Risk Loan home ownership': risk_loans['home_ownership'].value_counts(),\n",
    "    'Charged Off home ownership': charged_off_loans['home_ownership'].value_counts()\n",
    "    }\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    print(f\"Paid Loans home ownership: {index} {row['Paid home ownership']}\")\n",
    "    # print(f\"On time home ownership: {index} {row['On Time home ownership']}\")\n",
    "    # print(f\"Grace Period home ownership: {index} {row['Grace Period home ownership']}\")\n",
    "    print(f\"Risk Loan home ownership: {index} {row['Risk Loan home ownership']}\")\n",
    "    print(f\"Charged Off Loan home ownership: {index} {row['Charged Off home ownership']}\")\n",
    "\n",
    "df.plot(kind='bar', stacked=True)\n",
    "\n",
    "plt.title('Counts of Home Ownership Status')\n",
    "plt.xlabel('Home Ownership')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next plot \n",
    "data = {'Paid employment length': paid_loans['employment_length'].value_counts(),\n",
    "    # 'On Time employment length': on_time_loans['employment_length'].value_counts(),\n",
    "    # 'Grace Period employment length': grace_period_loans['employment_length'].value_counts(),\n",
    "    'Risk Loan employment length': risk_loans['employment_length'].value_counts(),\n",
    "    'Charged Off employment length': charged_off_loans['employment_length'].value_counts()\n",
    "    }\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    print(f\"Paid Loans employment length {index}: {row['Paid employment length']}\")\n",
    "    # print(f\"On time employment length {index}: {row['On Time employment length']}\")\n",
    "    # print(f\"Grace Period employment length {index}: {row['Grace Period employment length']}\")\n",
    "    print(f\"Risk Loan employment length {index}: {row['Risk Loan employment length']}\")\n",
    "    print(f\"Charged Off Loan employment length {index}: {row['Charged Off employment length']}\")\n",
    "\n",
    "df.plot(kind='bar', stacked=True)\n",
    "\n",
    "plt.title('Length of Employment')\n",
    "plt.xlabel('Employment Length')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next plot\n",
    "data = {\n",
    "'Paid annual income': paid_loans['annual_inc'],\n",
    "'Risk Loan annual income': risk_loans['annual_inc'],\n",
    "'Charged Off annual income': charged_off_loans['annual_inc']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Print the counts for each loan category\n",
    "for column in df.columns:\n",
    "    counts = df[column].value_counts()\n",
    "    print(f\"{column} counts:\")\n",
    "    print(counts)\n",
    "    print()\n",
    "\n",
    "# Plotting histogram-density plots using seaborn\n",
    "for column in df.columns:\n",
    "    plt.figure(figsize=(10, 6))  # Set the figure size as needed\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(df[column], kde=True)\n",
    "    plt.title(f\"Histogram-Density Plot - {column}\")\n",
    "    plt.xlabel(\"Annual Income\")\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.xlim(0, 700000)  # Adjust the x-axis limits as needed\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.boxplot(y=df[column])\n",
    "    plt.title(f\"Box Plot - {column}\")\n",
    "    plt.ylabel(\"Annual Income\")\n",
    "\n",
    "    # Customising y-axis tick labels\n",
    "    plt.gca().get_yaxis().get_major_formatter().set_scientific(False)\n",
    "    plt.tight_layout()  # Adjust the spacing between subplots\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# different view of box plots for annual_inc\n",
    "\n",
    "data = {'Paid annual income': paid_loans['annual_inc'].value_counts(),\n",
    "        # 'On Time annual income': on_time_loans['annual_inc'].value_counts(),\n",
    "        # 'Grace Period annual income': grace_period_loans['annual_inc'].value_counts(),\n",
    "        'Risk Loan annual income': risk_loans['annual_inc'].value_counts(),\n",
    "        'Charged Off annual income': charged_off_loans['annual_inc'].value_counts()\n",
    "        }\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    print(f\"Paid Loans annual income {index}: {row['Paid annual income']}\")\n",
    "    # print(f\"On time annual income {index}: {row['On Time annual income']}\")\n",
    "    # print(f\"Grace Period annual income {index}: {row['Grace Period annual income']}\")\n",
    "    print(f\"Risk Loan annual income {index}: {row['Risk Loan annual income']}\")\n",
    "    print(f\"Charged Off Loan annual income {index}: {row['Charged Off annual income']}\")\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Plotting the box plot\n",
    "df.boxplot()\n",
    "plt.title('Distribution of Annual Income')\n",
    "plt.ylabel('Annual Income')\n",
    "plt.xticks(rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another view\n",
    "data = {\n",
    "    'Paid annual income': paid_loans['annual_inc'],\n",
    "    'Risk Loan annual income': risk_loans['annual_inc'],\n",
    "    'Charged Off annual income': charged_off_loans['annual_inc']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Print the counts for each loan category\n",
    "for column in df.columns:\n",
    "    counts = df[column].value_counts()\n",
    "    print(f\"{column} counts:\")\n",
    "    print(counts)\n",
    "    print()\n",
    "\n",
    "df.boxplot()\n",
    "\n",
    "# Customizing the plot\n",
    "plt.title('Distribution of Annual Income')\n",
    "plt.ylabel('Annual Income')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Summary:\n",
    "\n",
    "# Grade / Sub Grade:\n",
    "# Those with a higher Grade or Sub grade were more likely to pay back the full loan, than those with lower grades\n",
    "# As the grade decends from 'A', there is a decrease in fully paid loans and an increase in risk and charged off loans between 'B1-E2'.\n",
    "# Thus, the lower the grade the more risky the loan becomes.\n",
    "\n",
    "# Annual Income:\n",
    "# in comparison to the paid loans, charged off and risk loans are drastically smaller\n",
    "# Thus indicating that a higher income is suggestive of being able to pay the full loan back\n",
    "# And smaller annual incomes are suggestive of needing more time/assistance or likely to be charged off \n",
    "\n",
    "# Purpose:\n",
    "# debt_consolidation Has a significant increase within the risk and charged off loans as a the primary/majority purpose for their loans\n",
    "# The majority of paid loans was for this purpose too\n",
    "# Meaning purpose alone may not directly be linked to likelyhood to be able to pay back the loans, an analysis of all indicators would need to be observed\n",
    "# Other than that small businesses and credit_card purposes also poses a strong likelyhood of being charged off\n",
    "\n",
    "# Home Ownership:\n",
    "# Morgage owners are very likly to be able to pay the full loan\n",
    "# Renters are slightly more likely to be charged off, but are still hold a good place for repaying their loans in full\n",
    "# however, people who own their property seem to be not as likely as the previous two to pay the loan in full\n",
    "# but there is less data on that point\n",
    "# so in summary people who rent their property should be accepted with some caution \n",
    "\n",
    "# Employment Length:\n",
    "# The largest amount of paid, charged off and risk loans all come frome people who have been in employment for 10 years+\n",
    "# But the paid loans are significantly higher, suggesting that people in long term employment are more likely to pay their loans in full.\n",
    "# However, ignoring the 10years+ most of the columns seem largely similar\n",
    "# This could suggest that employment length doesnt have a significant impact on ability to repay loans\n",
    "# but it should still be noted that people who have worked less than a year have the 2nd largest amount of charged off loans after the 10years+\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
